{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "195ff517-a407-4299-8b3c-f43ed5affbd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "exp1: max seq, projectors, and single-call classification\n",
    "literature survey - classification; a little more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c010772e-c76e-4130-9214-800b97b7980d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /databricks/python3/lib/python3.11/site-packages (4.41.2)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/f2/3a/8bdab26e09c5a242182b7ba9152e216d5ab4ae2d78c4298eb4872549cd35/transformers-4.47.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from transformers) (3.13.4)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.24.0 from https://files.pythonhosted.org/packages/61/8c/fbdc0a88a622d9fa54e132d7bf3ee03ec602758658a2db5b339a65be2cfe/huggingface_hub-0.27.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/22/06/69d7ce374747edaf1695a4f61b83570d91cc8bbfc51ccfecf76f56ab4aac/tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /databricks/python3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/10.1 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m9.7/10.1 MB\u001b[0m \u001b[31m137.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/450.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.23.4\n",
      "    Not uninstalling huggingface-hub at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862\n",
      "    Can't uninstall 'huggingface-hub'. No files were found to uninstall.\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.0\n",
      "    Not uninstalling tokenizers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862\n",
      "    Can't uninstall 'tokenizers'. No files were found to uninstall.\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.41.2\n",
      "    Not uninstalling transformers at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862\n",
      "    Can't uninstall 'transformers'. No files were found to uninstall.\n",
      "Successfully installed huggingface-hub-0.27.0 tokenizers-0.21.0 transformers-4.47.1\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Requirement already satisfied: datasets in /databricks/python3/lib/python3.11/site-packages (2.19.1)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/d7/84/0df6c5981f5fc722381662ff8cfbdf8aad64bec875f75d80b55bfef394ce/datasets-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from datasets) (3.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.11/site-packages (from datasets) (1.23.5)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=15.0.0 from https://files.pythonhosted.org/packages/5e/b5/9e14e9f7590e0eaa435ecea84dabb137284a4dbba7b3c337b58b65b76d95/pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /databricks/python3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Obtaining dependency information for requests>=2.32.2 from https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Obtaining dependency information for tqdm>=4.66.3 from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /databricks/python3/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /databricks/python3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /databricks/python3/lib/python3.11/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /databricks/python3/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.11/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m389.1/480.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/40.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/40.1 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/40.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/40.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/40.1 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/40.1 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/40.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/40.1 MB\u001b[0m \u001b[31m138.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/40.1 MB\u001b[0m \u001b[31m161.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m30.3/40.1 MB\u001b[0m \u001b[31m161.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m35.9/40.1 MB\u001b[0m \u001b[31m160.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m71.7/78.5 kB\u001b[0m \u001b[31m161.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, requests, pyarrow, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Not uninstalling tqdm at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862\n",
      "    Can't uninstall 'tqdm'. No files were found to uninstall.\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Not uninstalling requests at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.1\n",
      "    Not uninstalling pyarrow at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862\n",
      "    Can't uninstall 'pyarrow'. No files were found to uninstall.\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.19.1\n",
      "    Not uninstalling datasets at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862\n",
      "    Can't uninstall 'datasets'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "petastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.2.0 pyarrow-18.1.0 requests-2.32.3 tqdm-4.67.1\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Requirement already satisfied: accelerate in /databricks/python3/lib/python3.11/site-packages (0.31.0)\n",
      "Collecting accelerate\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/c2/60/a585c806d6c0ec5f8149d44eb202714792802f484e6e2b1bf96b23bd2b00/accelerate-1.2.1-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /databricks/python3/lib/python3.11/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /databricks/python3/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /databricks/python3/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /databricks/python3/lib/python3.11/site-packages (from accelerate) (2.3.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from accelerate) (0.27.0)\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Obtaining dependency information for safetensors>=0.4.3 from https://files.pythonhosted.org/packages/e6/ee/69e498a892f208bd1da4104d4b9be887f8611bf4942144718b6738482250/safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.5.0)\n",
      "Requirement already satisfied: requests in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: sympy in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /databricks/python3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/336.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m256.0/336.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/435.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m430.1/435.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, accelerate\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.2\n",
      "    Not uninstalling safetensors at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862\n",
      "    Can't uninstall 'safetensors'. No files were found to uninstall.\n",
      "  Attempt\n",
      "\n",
      "*** WARNING: max output size exceeded, skipping output. ***\n",
      "\n",
      "al_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from peft) (4.47.1)\n",
      "Requirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from peft) (1.2.1)\n",
      "Requirement already satisfied: safetensors in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from peft) (0.27.0)\n",
      "Requirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.25.0->peft) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.25.0->peft) (2023.5.0)\n",
      "Requirement already satisfied: requests in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.25.0->peft) (4.10.0)\n",
      "Requirement already satisfied: sympy in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: networkx in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /databricks/python3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.82)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.11/site-packages (from transformers->peft) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m286.7/374.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.14.0\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting trl\n",
      "  Obtaining dependency information for trl from https://files.pythonhosted.org/packages/01/4b/73924544701445ee8cb55cce80a5917242d6f8a6f16c76408e29111209d7/trl-0.13.0-py3-none-any.whl.metadata\n",
      "  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from trl) (1.2.1)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from trl) (3.2.0)\n",
      "Requirement already satisfied: rich in /databricks/python3/lib/python3.11/site-packages (from trl) (13.7.1)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from trl) (4.47.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /databricks/python3/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (23.2)\n",
      "Requirement already satisfied: psutil in /databricks/python3/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /databricks/python3/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /databricks/python3/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (2.3.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (0.27.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (0.4.5)\n",
      "Requirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.13.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /databricks/python3/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (0.3.6)\n",
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /databricks/python3/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /databricks/python3/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /databricks/python3/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /databricks/python3/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.8.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.11/site-packages (from transformers>=4.46.0->trl) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /databricks/python3/lib/python3.11/site-packages (from rich->trl) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.11/site-packages (from rich->trl) (2.15.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.10.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /databricks/python3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.11.1)\n",
      "Requirement already satisfied: networkx in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1)\n",
      "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /databricks/python3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.34.0->trl) (12.5.82)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.11/site-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas->datasets>=2.21.0->trl) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.21.0->trl) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Downloading trl-0.13.0-py3-none-any.whl (293 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/293.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m235.5/293.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: trl\n",
      "Successfully installed trl-0.13.0\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting bitsandbytes\n",
      "  Obtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/99/9a/f41d252bf8b0bc5969b4dce1274cd04b7ddc541de1060dd27eca680bc1b2/bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata\n",
      "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: torch in /databricks/python3/lib/python3.11/site-packages (from bitsandbytes) (2.3.1+cu121)\n",
      "Requirement already satisfied: numpy in /databricks/python3/lib/python3.11/site-packages (from bitsandbytes) (1.23.5)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /databricks/python3/lib/python3.11/site-packages (from bitsandbytes) (4.10.0)\n",
      "Requirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (3.13.4)\n",
      "Requirement already satisfied: sympy in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (1.11.1)\n",
      "Requirement already satisfied: networkx in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (2023.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /databricks/python3/lib/python3.11/site-packages (from torch->bitsandbytes) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /databricks/python3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.11/site-packages (from jinja2->torch->bitsandbytes) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.11/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/69.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:14\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/69.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\n",
      "\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/69.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/69.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/69.1 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/69.1 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/69.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/69.1 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/69.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/69.1 MB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/69.1 MB\u001b[0m \u001b[31m162.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/69.1 MB\u001b[0m \u001b[31m164.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/69.1 MB\u001b[0m \u001b[31m164.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/69.1 MB\u001b[0m \u001b[31m165.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/69.1 MB\u001b[0m \u001b[31m166.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m53.6/69.1 MB\u001b[0m \u001b[31m165.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m59.4/69.1 MB\u001b[0m \u001b[31m166.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m65.0/69.1 MB\u001b[0m \u001b[31m166.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.0\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# based on https://www.datacamp.com/tutorial/fine-tuning-llama-3-2\n",
    "\n",
    "%pip install -U transformers \n",
    "%pip install -U datasets \n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes \n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "944cf1a3-d315-4daa-8fdf-96cfe91e8d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 03:39:55.594238: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-24 03:40:03,712] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    LlamaForCausalLM\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer, setup_chat_format, SFTConfig\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt \n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d5d03c-d528-4453-af93-2788e3994d61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "base_dir = \"123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b45d93dc-b5f0-4b51-8ea0-9b64017bbae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6701a4e6-e0b9-4560-afee-e79eb8069916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b4b3484bfb493990aaf6eb9cdd10f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e5a611d6344f85bd92922d191c8a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba436760307f4d14968ae575623296dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    %pip install -qqq flash-attn\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     base_model, quantization_config=bnb_config, device_map='auto',attn_implementation=attn_implementation\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "212e6c63-ac32-4b37-886a-d4dfa5cfa199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773fe34476f04fc092cd539d99418bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e61f4ba66a47488fc81e1d80047b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f3814342774fa88e68a94944fc3aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None:\n",
    "    tokenizer.chat_template = None  # Reset the chat template\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "#tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e12b9704-c234-4d6e-a9a7-9d406c96274f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# adding adapter\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    #target_modules=['q_proj', 'v_proj'] \n",
    "    target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8341a43e-bc82-423e-afb1-dc9b7aa41e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/utils.py:117: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [LOSKEY] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "df = spark.read.parquet(\"/bdfs/FileStore/churn/test1\")\n",
    "df = df.select(\"*\").toPandas()\n",
    "df = df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bdc0f2f-801a-4ca3-9ab4-cb49fc80f884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# put training data into prompts and set up word limit\n",
    "\n",
    "def df_parser(raw_pandas, word_limit):\n",
    "  df = pd.DataFrame() \n",
    "  for index, rows in raw_pandas.iterrows():\n",
    "    chat_history = \"\"\n",
    "    for i in range(1,11): #don't include the 1st chat\n",
    "      if rows[f\"transcript_{i}\"] != \"NA\":\n",
    "        chat_history += f\"### New Chat Session ###\" + rows[f\"transcript_{i}\"] + \"#########\" + \"\\n\\n\"\n",
    "    word_list = chat_history.split()\n",
    "    word_count = len(word_list)\n",
    "\n",
    "    if rows[\"disconnect_date\"] != \"NA\":\n",
    "      label = \"Yes\"\n",
    "    else: label = \"No\"\n",
    "\n",
    "    df.at[index, \"label\"] = label\n",
    "\n",
    "    if word_count <= word_limit:\n",
    "      instruction = prompt_template_instruction(chat_history)\n",
    "      df.at[index, \"text\"] = instruction + label + \"<|eot_id|>\"\n",
    "      df.at[index, \"testing_script\"] = instruction\n",
    "    else:\n",
    "      chat_history = \" \".join(word_list[:word_limit])\n",
    "      instruction = prompt_template_instruction(chat_history)\n",
    "      df.at[index, \"text\"] = instruction + label + \"<|eot_id|>\"\n",
    "      df.at[index, \"testing_script\"] = instruction\n",
    "  return df\n",
    "\n",
    "def prompt_template_instruction(conversation_history): #llama2 prompt standard: https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-2/\n",
    "  return \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a call center analyst. Your task is to identy whether a caller's chat history with agents would indicate their potential disengagement/churn from the service in the future.<|eot_id|><|start_header_id|>user<|end_header_id|>\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577a2cfe-e376-4872-a64e-38b3b38a8088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_total = df_parser(df,word_limit=12000)\n",
    "df = df_total[0:5000]\n",
    "df_inference = df_total[5000:]\n",
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd8f9b0-f62d-432d-bc6e-9862e2be59f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37eecc6a-cd22-4b27-b7e6-5121b0fc8ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 2\n",
    "optim = 'adamw_hf'\n",
    "learning_rate = 1e-5\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"linear\"\n",
    "num_train_epochs = 3\n",
    "max_seq_length = 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1f1ee3d-4767-4a10-b556-884d2f219307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddae6d0b910e4d60b56169b66d3e380b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563eb33bbd654b84bff9bae694ed0bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 53:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.018688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.016223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.016259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.016056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.015897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.015877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.015907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.016018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.015893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.015864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.015890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.015896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.05262324754397074, metrics={'train_runtime': 3190.7527, 'train_samples_per_second': 3.761, 'train_steps_per_second': 1.88, 'total_flos': 9659839131156480.0, 'train_loss': 0.05262324754397074, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,  \n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    peft_config = peft_config,\n",
    "    args = SFTConfig(\n",
    "      learning_rate = 1e-5, #2e-4,  \n",
    "      num_train_epochs = num_train_epochs,\n",
    "      output_dir = base_dir,\n",
    "      max_seq_length = max_seq_length,\n",
    "      per_device_train_batch_size = per_device_train_batch_size,\n",
    "      gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "      use_liger = True,\n",
    "      save_strategy = \"steps\",\n",
    "      eval_strategy = \"steps\",\n",
    "      lr_scheduler_type = lr_scheduler_type,\n",
    "      optim = optim\n",
    "      )   \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69efbab-bdd0-4579-92b4-f90521e5284c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-1000/', name='checkpoint-1000/', size=0, modificationTime=1734729070000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-1350/', name='checkpoint-1350/', size=0, modificationTime=1734729183000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-1500/', name='checkpoint-1500/', size=0, modificationTime=1734734091000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-2000/', name='checkpoint-2000/', size=0, modificationTime=1734734252000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-2500/', name='checkpoint-2500/', size=0, modificationTime=1734734412000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-3000/', name='checkpoint-3000/', size=0, modificationTime=1734734571000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-3500/', name='checkpoint-3500/', size=0, modificationTime=1734734730000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-4000/', name='checkpoint-4000/', size=0, modificationTime=1734734890000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-4500/', name='checkpoint-4500/', size=0, modificationTime=1734735050000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-500/', name='checkpoint-500/', size=0, modificationTime=1734728911000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-5000/', name='checkpoint-5000/', size=0, modificationTime=1734735210000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-5500/', name='checkpoint-5500/', size=0, modificationTime=1734735369000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/checkpoint-6000/', name='checkpoint-6000/', size=0, modificationTime=1734735530000),\n",
       " FileInfo(path='dbfs:/FileStore/churns/llama-3.2-1b/cache/runs/', name='runs/', size=0, modificationTime=1734728755000)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"FileStore/churns/llama-3.2-1b/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb61e75-a3a5-4e06-98fc-eb7831727f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pause to decide the best model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03fb35d1-65e2-4c3d-98f8-2b07a4585c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "peft_model_checkpoint = 'cache/checkpoint-3000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b4a83a-9ad6-454a-8bcd-8f102ab9bb80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-151f4588-5515-4975-9984-52aca1cc4862/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:396: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n",
      "```python\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "# Load original tied model\n",
      "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n",
      "\n",
      "# Set the randomly initialized lm_head to the previously tied embeddings\n",
      "model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
      "\n",
      "# Save the untied model\n",
      "untied_model_dir = \"dir/for/untied/model\"\n",
      "model.save_pretrained(untied_model_dir)\n",
      "model.config.save_pretrained(untied_model_dir)\n",
      "\n",
      "# Now use the original model but in untied format\n",
      "model = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n",
      "```\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the checkpoint and merge\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "base_model_reload= AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "#peft_model = PeftModel.from_pretrained(model, peft_model_checkpoint)\n",
    "\n",
    "\n",
    "if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None:\n",
    "    tokenizer.chat_template = None  # Reset the chat template\n",
    "\n",
    "base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model_reload, peft_model_checkpoint)\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c67f1b-e5ce-4675-a738-9eb0a95173b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: Yes\n",
      "answer: Yes\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n",
      "answer: No\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for index, row in df_inference.iterrows():\n",
    "  inputs = tokenizer(row[\"testing_script\"], return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "  outputs = model.generate(**inputs, max_new_tokens=1, num_return_sequences=1)\n",
    "  text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  answer = text.split(\"assistant\")[1]\n",
    "  print(f\"answer: {answer}\")\n",
    "  pred.append(answer)\n",
    "  #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57c4dbb7-5c72-4671-98ee-af9bdb201411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.56\n",
      "Recall: 0.41\n",
      "F1 Score: 0.48\n",
      "AUC: 0.54\n"
     ]
    }
   ],
   "source": [
    "gold = df_inference[\"label\"].tolist()\n",
    "\n",
    "# Sample prediction and gold_answer \n",
    "prediction = [1 if x.lower() == \"yes\" else 0 for x in pred]\n",
    "gold_answer = [1 if x.lower() == \"yes\" else 0 for x in gold]\n",
    "\n",
    "# Compute Precision, Recall, F1 Score \n",
    "precision = precision_score(gold_answer, prediction) \n",
    "recall = recall_score(gold_answer, prediction) \n",
    "f1 = f1_score(gold_answer, prediction) \n",
    "\n",
    "# Compute ROC Curve and AUC \n",
    "fpr, tpr, _ = roc_curve(gold_answer, prediction) \n",
    "roc_auc = auc(fpr, tpr) \n",
    "\n",
    "print(f\"Precision: {precision:.2f}\") \n",
    "print(f\"Recall: {recall:.2f}\") \n",
    "print(f\"F1 Score: {f1:.2f}\") \n",
    "print(f\"AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an MLflow run \n",
    "with mlflow.start_run(): \n",
    "  \n",
    "  # Log metrics \n",
    "  mlflow.log_metric(\"precision\", precision) \n",
    "  mlflow.log_metric(\"recall\", recall) \n",
    "  mlflow.log_metric(\"f1_score\", f1) \n",
    "  mlflow.log_metric(\"roc_auc\", roc_auc) \n",
    "  \n",
    "  # Log ROC Curve \n",
    "  plt.figure() \n",
    "  plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})') \n",
    "  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') \n",
    "  plt.xlim([0.0, 1.0]) \n",
    "  plt.ylim([0.0, 1.05]) \n",
    "  plt.xlabel('False Positive Rate') \n",
    "  plt.ylabel('True Positive Rate') \n",
    "  plt.title('Receiver Operating Characteristic') \n",
    "  plt.legend(loc=\"lower right\") \n",
    "  plt.savefig(\"/dbfs/FileStore/roc_curve.png\") \n",
    "  plt.show() \n",
    "  \n",
    "  # Log ROC curve image \n",
    "  mlflow.log_artifact(\"/dbfs/FileStore/roc_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf83ab2-3fc7-4e12-a487-30b52b113e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/dbfs/FileStore/churns/llama-3.2-1b/model/tokenizer_config.json',\n",
       " '/dbfs/FileStore/churns/llama-3.2-1b/model/special_tokens_map.json',\n",
       " '/dbfs/FileStore/churns/llama-3.2-1b/model/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = \"/dbfs/FileStore/churns/llama-3.2-1b/model\"\n",
    "\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "experiment 1 llama3 1b HF version 2 working",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
